{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../data/normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 341400 entries, 0 to 341399\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    340935 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert all entries in the 'text' column to strings\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Step 2: Define a function to split long sentences\n",
    "def split_long_sentences(texts, max_length=100000):\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        while len(text) > max_length:\n",
    "            split_point = text.rfind(' ', 0, max_length)\n",
    "            if split_point == -1:\n",
    "                split_point = max_length\n",
    "            new_texts.append(text[:split_point])\n",
    "            text = text[split_point:].strip()\n",
    "        new_texts.append(text)\n",
    "    return new_texts\n",
    "\n",
    "# Step 3: Process data in chunks\n",
    "chunk_size = 1000  # Adjust chunk size based on memory constraints\n",
    "output_files = []\n",
    "\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df['text'][i:i + chunk_size]\n",
    "    preprocessed_chunk = split_long_sentences(chunk, max_length=100000)\n",
    "    \n",
    "    output_file = f'preprocessed_chunk_{i // chunk_size}.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        for line in preprocessed_chunk:\n",
    "            f.write(line + '\\n')\n",
    "    output_files.append(output_file)\n",
    "\n",
    "# Step 4: Create a combined iterator from all preprocessed files\n",
    "def combined_iterator(files):\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                yield line.strip()\n",
    "\n",
    "# Step 5: Use the combined iterator to train the SentencePiece model\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=combined_iterator(output_files),\n",
    "    model_prefix='amh_tokenizer_model',\n",
    "    vocab_size=1000,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    max_sentence_length=100000  # Set to maximum allowed length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='amh_tokenizer_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 217,\n",
       " 501,\n",
       " 903,\n",
       " 583,\n",
       " 6,\n",
       " 617,\n",
       " 66,\n",
       " 11,\n",
       " 138,\n",
       " 106,\n",
       " 104,\n",
       " 42,\n",
       " 30,\n",
       " 45,\n",
       " 111,\n",
       " 389,\n",
       " 8,\n",
       " 4,\n",
       " 34,\n",
       " 18,\n",
       " 148,\n",
       " 91]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode('የአዲስ ዘመን ጋዜጣ ቀደምት ዘገባዎች በእጅጉ ተነባቢ ዛሬም ላገኛቸው')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = sp.encode('የአዲስ ዘመን ጋዜጣ ቀደምት ዘገባዎች በእጅጉ ተነባቢ ዛሬም ላገኛቸው')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'የአዲስ ዘመን ጋዜጣ ቀደምት ዘገባዎች በእጅጉ ተነባቢ ዛሬም ላገኛቸው'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode(encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
